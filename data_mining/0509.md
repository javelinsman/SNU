#### k-means Clustering

* Euclidean space/distance를 가정
* 일단 k를 정하고
* 각 cluster별로 점 하나 할당
    * 랜덤하게 결정하거나
    * 아님 첫 점 대충 결정하고 최대한 먼 거 잡는 과정 반복
* 각 점은 제일 가까운 centroid에 배정하고, centroid 업뎃하는거 반복
* 점들이 클러스터 변화 없고, 센트로이드 안정화되면 수렴했다고 판정
* k는 어케 정하지
    * finding the knee 방법
    * 걍 여러 k를 해보면서 centroid까지 평균거리가 그닥 안 줄어드는 지점 찾기
        * ㄴ자 모양 커브

#### BFR algorithm

* extension of k-means to large data
    * disk-resident data sets
* centroid 중심으로 cluster 내의 점들이 multivariate distribution으로 분포되어 있다고 가정함
    * clusters는 axis-aligned ellipses다 (회전ㄴㄴ)
* 메모리가 O(clusters)만큼 든다
* 걍 one-pass로 점들 읽으면서 이전에 읽었던 점들은 simple statistics로 보관
* 처음에 일단 k centroid 정하기
    * 랜덤
    * 하나 정하고 최대한 멀리
    * 작은 random sample 가지고 clustering 해보기
* 점들을 세 개의 분류로 관리
    * DS(discard set) : centroid에 충분히 가까운 것들
        * 각 cluster마다 N, SUM, SUMSQ로 관리함
        * SUM = sum pi, SUMSQ = sum pi*pi (elementwise multiplication)
            * 둘 다 벡터요
            * 왜 필요하죠? 평균이랑 분산 구하려구요
        * 차원이 d라 하면 2d+1개의 값이 필요함
    * CS(compression set) : centroid엔 안가까운데 지들끼리 가까운거
    * RS(retained set) : 이도저도 아닌 걍 점, isolated point
* how?
    * 일단 각 cluster에 충분히 가까운 점을 찾아서 DS에 넣음
    * 그리고 나머지 점들 대충 클러스터링 해서 CS랑 RS(outlier) 정함
    * DS의 N, SUM, SUMSQ 업뎃
    * CS들 merge 해봄
    * 다 끝났으면 CS랑 RS 전부 가까운 cluster에 넣음
* centroid에 충분히 가까운지는 어떻게 판별하니
    * likelihood 높은 거 고를 수도 있고
    * Mahalanobis Distance가 t보다 작은거 (t는 parameter) 고를수도
    * d(x,c) = sqrt(sum [(xi - ci)/sigma_i]^2)
        * 즉 차원별로 normalized 거리를 구하고 rms
        * 이러면 표준편차가 하나가 되는데 sqrt(d)임
        * 등고선이 타원모양으로 나온대
* CS 두개 합치는 기준은?
    * combined cluster의 variance가 s보다 작으면

#### CURE algorithm

* BFR이나 k-means는 normal distribution을 가정하고 축이 고정됨
