#### Itemsets : computation model

* 자 그래서 freq itemset을 어떻게 찾을까요
* 전형적으로 data는 DB가 아니라 flat file에 저장되어 있음
    * stored on disk, basket-by-basket
    * basket은 작음
* k nested loop를 돈다고?
* computation model
    * disk-resident data의 true cost는 the number of disk I/O
    * association-rule algorithms read the data in passes
    * measure cost by the number of passes
    * 그래서 막 세어야 하는데 main memory에 count를 다 담는게 병목
* finding frequent pairs
    * pair가 frequent한 경우가 제일 많기 때문에 pair가 제일 어려워요
    * pair부터 세어 봅시다
    * 모든 itemset을 generate하되, 결국 자주 나올 것만 세고 싶네요
* Naive algorithm
    * 파일 한번 읽고, 2중 for문 돌아서 n(n-1)/2개 만들자
    * item 종류가 N이면 최대 N(N-1)/2 개가 나올수 있으니 메모리에 담기 너무 큼
    * Approaches
        1. count all pairs using a matrix
            * 4 bytes per pair -> N(N-1)/2 * 4 = 2 N^2 bytes
            * 대각성분 제외하고 a1^T a2^T ... 식으로 나열하면
            * **j행 i열** index는 sum_k=1^i-1 (n-k) + (j-1)
                * = (i-1)(n-i/2) + j-i
        1. Keep a table of triples [i, j, c]
            * 12 bytes per pair
            * hash[i,j] = c;
            * but only for pairs with count > 0
            * 1번 이상 등장하는 pair가 가능한 경우의 수의 1/3보다 작을 때 유용
                * 3 M < N^2

#### A-Priori algorithm

* two-pass approach
* key idea : monotonicity ({A} vs. {A, B})
    * item i가 s번 안나타나면 i를 포함하는 모든 pair도 freq하지 않음
* how?
    * pass 1 : 각 item의 occurence를 센다
    * pass 2 : freq item으로 구성된 pair다 생성하고 쭉 돌면서 센다
    * (# freq item)^2 + (list freq item) 만큼 메모리가 듬
    * 좌표압축하고 approach 1 쓰면 댐
* k-tuple일 땐?
    * C_k : candidate k-tuples
    * L_k : true freq k-tuples
    * C1 -(filter)-> L1 -(construct)-> C2 -(filter)-> L2 -(construct)-> C3 ->...
* L2 -> C3 어케함
    * {a, b}, {a, c} 꼴의 tuple 두개 합치고 {b, c}도 freq한지 확인하기
    * L2 sort하면 더 쉽게 가능
* properties
    * one pass for each k
    * 각 k-tuple들 셀 정도로 memory 필요
    * 보통 support 1%정도로 했을 떄 k=2가 제일 메모리 많이 먹음
* extension
    * association rules with intervals
    * association reuls when items are in a taxonomy
    * lower the min. support s as itemset gets bigger

#### PCY algorithm

* pass 1에서 메모리 많이 남으니까 
    * 거기서 추가적인 작업을 해서 더 효율적이게 해보자
    * 출현하는 모든 pair에 대해 hash(pair)++를 함
    * 그 값이 s 이상인지 아닌지로 bit vector 만듬
* pass 2에서
    * i, j가 freq하고
    * bitmap[hash(i,j)] == 1일 때만 count
* {item counts, hash table for pairs} -> {freq items, bitmap, count of cand. pair}
* s보다 더 셀 필요가 없으니 bucket은 s를 담을 만큼이면 충분
    * s < 256이면 1byte
* #bucket = O(main memory size)
    * 최대한 많아야 false positive 늘어나겠지
* refinement : multistage algorithm
    * 걍 같은 짓 새로운 hash function으로 한번 더함
    * 두 hash 함수는 독립적이어야겠지
    * 3 pass로 늘어나는 대신 false positive 줄어듬
    * 마지막에 체크할때도 overhead 생김
        * (freq[i] & freq[j] & b1[h1(i,j)] & b2[h2(i,j)]) == 1
* refinement : multihash
    * 위에 거를 그냥 2-pass에 함
    * 대신 hash table size를 절반정도로 해서 나눠가짐
    * table size 줄어드는게 risk이긴 함
* extension
    * 각각 hash n개 써도 됨
    * multistage에서 diminishing return point가 있음
        * bit vector가 메모리 꽉 채워버리면 안되니까
    * multihash에서 넘 많으면 risk때매 힘듬
