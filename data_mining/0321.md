#### cost measures for algorithms

* we have multiple machines
    * we should specify the number of machines in O notation
* typical way
    * communication cost
        * to mapper, to reducer, to DFS
        * approximation
            * input file size (DFS -> Mapper)
            * + 2 * (sum of the sizes of all files from M to R)
                * M -> local disk of M -(networdk) -> local disk of R
            * + (sum of the output sizes of the R)
    * elapsed communication cost
        * critical path of communication cost
    * (elapsed) computation cost

* either I/O or processing costs dominates
    * word count : I/O

#### cost of Map-Reduce

* each cost
    * inp = |R| + |S|
    * int = |R| + |S|
    * out = |R natjoin S|
* total comm. cost = inp + 2 * int + out
    * = O(|R| + |S| + |R natjoin S|)
* elapsed comm. cost
    * = O(s)
    * if the number of mapper and reducer(k) is determined
    * then s can be calculated
    * k가 작아지면 s는 늘어남
* comp. cost는 보통 O(inp + out)
    * 그래서 여기서는 comm. cost랑 비슷

#### Reading

* [**MapReduce 최초 제안 논문**](http://static.googleusercontent.com/media/research.google.com/ko/archive/mapreduce-osdi04.pdf)
* [Google File System](http://static.googleusercontent.com/media/research.google.com/ko/archive/gfs-sosp2003.pdf)


# Finding Similar Items

* many problems = finding "similar" sets
    * near-neighbors in __high-dim__ space
    * ex.
        * scene completion (similar pic)
        * search engines or plagarism check (similar document)
        * customer who parchesed similar products

#### formal definition

* Given high-dim data x1, ..., xn
* and distance function d
* find all (xi, xj) s.t. d(xi, xj) <= s
* for some s
* can be done in O(n)! how?!

#### distance measures

* todays, Jaccard distance/similarity is used
    * Jaccard similarity sim(C1, C2) = |C1 교 C2| / |C1 합 C2|
    * Jaccard distance d(C1, C2) = 1 - sim(C1, C2)

#### lets find similar docs

* find near duplicate pairs, where n = millions or billions
* problems
    * 같은 단어, 문장이라도 순서는 다를 수 있음
    * n too large for O(n^2)
    * docs are so large/many to fit in main memory
* 3 steps
    1. shingling : doc -> set
    2. min-hasing : set -> sig, compress the sets while preserving sim.s
    3. locality-sensitive hasing : focus on candidate sig.s

#### Shinglings

* simple approaches
    * set of words appearing in doc
    * set of important words
    * but doesnt work well...
    * 단어 순서는 중요함
    * 문단 순서만 무시하자
* shingle!
    * k-shingle (or k-gram) = seq. of k tokens
        * token = char, words, ... 
        * depending on the app
    * set 말고 multiset 쓸 수도 있음
* compressing shingles
    * O(n-k)개의 shingle이 나오는데
    * 저장공간은 O(nn-nk)니까 그러지 말고
    * hash(k-shingle) -> 4byte 를 저장함
        * conflict가 최대한 안 나도록 해야겠지만
        * 뭐 조금 나는 정도는 크게 상관 없다
    * doc -> set of hash val of its k-shingles
* similarity metric
    * 모든 doc의 k-shingle들의 space에서 0/1 vector가 될 수도 있지만
    * 매우 sparse하므로
    * Jaccard similarity를 씁시다

#### real number example

* Doc1 = 10k bytes
* N = 1k words
* k = 10

* #shingles = N - k + 1 ~= 1k
* size of each shingle = 10 byte
* total size of shingles = 10k bytes

* Idea 1) h(shingles) -> 4 byte
* total shingle size = 4kbytes => 400 bytes <- ???

#### Minhash/LSH

* pair-wise하게 다 계산하면 느려요
    * n이 100만이면 5일 걸릴듯
* 일단 bit vector로 생각을 하죠
* (num shingle) x (num doc)으로 행렬표현 가능
    * 물론 매우 sparse
* find similar columns!
