##### fixed proportion

* search engine : (user, query, time)
* "how often did a user run the same query in a single day?"
* have space to store 1/10 of query stream
* naive solution : if randint(0, 9) == 0
* but consider the following query
    * "average search engine user -> frac. of duplicates?"
    * each user issues x queries once, d queries twice
        * total x+2d queries, the correct fraction is d/(x+d)
    * in the sample, the number of singletone queries is
        * x / 10
    * in the sample, the number of doubled queries is
        * not sampled : 81/100 d
        * sampled once : 18/100 d
        * sampled twice : 1/100 d
    * sample based answer is d /(10x + 19d)
        * it is wrong answer
    * solution : sample users
        * pick 1/10 of user and take all their queries
        * hash function : user name or id -> 10 buckets uniformly
* generalized solution
    * to get a sample of a/b fraction of the stream
    * hash each tuple.s key into b buckets
    * pick the tuple if its hash val. <= a

#### fixed-size sample

* sample size needs to be s
* suppose at time n we have seen n items
    * each item is in the sample S with equal prob s/n
* impractical solution : at each time pick s tuples at random
* Reservoir Sampling
    * store all the first s elem
    * now looking at n-th element
    * with prob. s/n -> pop random item and push the new item
    * else -> discard it
    * proof of uniformity
        * base case : if n =s then p = s/n = s/s = 1
        * inductive case : suppose n elems are stored with prob. s/n
            * elem in S to be kept : (1 - s/(n+1)) + s/(n+1)*(s-1/s) = n/(n+1)
            * at time n+1, it stayes with prob. s/n * n/(n+1) = s/(n+1)
            * new item is also pushed with prob. s/(n+1)

#### sliding windows

* queries are about a window of length N, most recent elements
* interesting case : N can be too large to store even on disk
* ex. Amaxon
    * every product X, keep 0/1 streams indicating that it was sold in n-th transaction
    * how many times have we sold X in the last k sales?
    * obvious solution : new one comes, the oldest one discarded
        * but N is too large...
        * let.s make just approximate answer
* simple solution
    * uniformity ussumption
    * s : number of 1s from the beginning of the stream
    * z : number of 0s from the beginning of the stream
    * N * s / (s+z)
    * but distribution changes over time in real world problem...
* DGIM method
    * does not assume uniformity
    * store O(log^2 N) bits per stream
        * log^2 N = (log N)^2
    * solution gives approximate answer, error smaller than 50%
        * |x - x.| / |x| < 0.5
    * how?
        * make bucket containing 2^x 1s, with its size exponentially increasing
        * record the timestamp % N, which needs O(log_2 N) bits
        * bucket : a record consisting of
            * the timestamp of its end, O(log N) bits
            * the number of 1s between beginning and end, O(log log N) bits
                * since the number of 1 is the power of 2
        * either one or two buckets with same number of 1
            * do not overlap
            * sorted by size
            * disappear when completely out of N recent elems
    * storage requirement
        * total number of buckets : O(log N)
        * each bucket requires O(log N) bits
        * thus total requirement is O(log^2 N)
    * updating buckets
        * when new bit comes in, drop the last bucket if necessary
        * if new bit is 0 : continue
        * if new bit is 1
            * create a new bucket of size 1
            * if there are three buckets or size 1, combine oldest two
            * repeat the procedure until the constraint holds
