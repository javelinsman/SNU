## Importance of Words in Documents

Term Frequence - Inverse Document Frequency (TF.IDF)

* TF : 자주 등장하면 중요한 단어
    * 하지만 a, the 같은 걸 제외해야...
    * TF_ij = {f_ij} over {**max**_k f_kj}
    * normalize 하는 이유? 문서 길이가 각각 다를 수 있어서
    * 근데 왜 sum이 아니고 max로 나누지?
    * 교수님 답변 : 보통 power law를 따라서 1등 단어가 훨씬 많아서 별 차이가 없을 것...
* IDF : 그 문서에서만 등장하는 단어가 더 중요한 단어
    * IDF_i = log_2 (N / n_i)
    * log를 취하는 이유? 너무 커서...
    * TF는 0~1 사이 값이라서 IDF도 좀 줄여줘야
* TF.IDF score of term i in doc.j = TF_ij * IDF_i

## Hash Functions

Modular hash function : h(x) = x mod k
    * k는 소수여야 equally distribute가 잘 된다
    * (2, 4, 6, 8, 10, 12) mod 10 vs. 11

## Index

각 data row를 특정 attribute(s)에 대하여 associate list를 만듬

* B-tree
* Hash
    * 링크드 리스트에 기록들을 담음
    * storage requirement : n*m + (the size of pointer, say 4) * (n + B)

## Secondary Storage

Memory vs. Disk trade-off

* Disk보다 memory가 비쌈
* Disk(~10ms)가 memory보다 10^5배 정도 느림
* Disk는 블록(64KB) 단위로 불러와서 임의 접근보다 순차 접근이 더 좋음
* 알고리즘 설계할 때 웬만하면 memory 쓰고 disk 써야 하면 순차 접근 하세요

강의노트에 1 day라고 나온 거 근사치임. 10^5 맞음

## Base of Natural Log

* a가 작을 때 다음의 근사 식이 유용하다
    * (1+a)^b ~= e^ab
    * (1-a)^b ~= e^-ab
* e^x의 근사식 : 1 + x + x^2 / 2 + x^3 / 6 + ...

## Power Law

* Linear relationship between the logarithms of two variables
* log(y) = a log(x) + b
    * <=> y = c x^a
* long tail
    * average << max possible value
    * unlike gaussian, where average ~= max(min) possible value
* 자연계의 많은 것들이 이 법칙을 따른다
    * 사람들 평균 연봉이 30K라고 하면 다들 가우시안 생각하고 그러려니 하는데 사실 power law라서 부자킹들이 많다...
    * Facebook likes of famous people
    * Node degree(the number of incoming edge) in the web graph
* Zipf.s law : y = cx^-1/2
    * word frequencies(y) - text(x)
