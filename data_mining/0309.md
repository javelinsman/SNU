## MapReduce

MapReduce : distributed computing을 위한 google의 computation/data manipulation model

### 계산 모델의 발전 과정

1. CPU - Memory architecture
    * for Algorithms, ML, Statistics
1. CPU - Memory & Disk architecture
    * for classical data maining
    * sequential access
    * but, Google.s web page : 20+ billion * 20KB = 400+ TB
        * 하나의 컴퓨터로는 읽는 데 4개월
        * 저장하는 데 hard drive 1000개
1. cluster architecture
    * 여러개의 machine을 동시에 사용
    * 하나의 rack에 16-64개의 node를 집어넣음
    * 강의노트 6p 그림 참조

### challenges

데이터 마이닝은 large scale computing을 필요로 하므로 다음의 challenge들이 있음.
1. 어떻게 분배할 것인가
1. distributed program을 어떻게 구현할 것인가?
1. machine fail
    * 서버 하나가 3년(1000일) 동안 작동할 수 있다고 하면
    * 서버가 1000개면 하루에 하나씩 고장
    * 구글은 서버가 100만개인데...
1. data 복사도 오래 걸림
    * 해결책?
    * 데이터가 있는 곳에 코드를 보낸다(bring computation close to data)
    * 서버 고장에 대처하기 위해서 data를 중복해서 저장

### storage infrastructure

Distributed File System (DFS)

* global file namespace
    * 노드 하나에 디스크가 500GB인데 1TB짜리 텍스트를 저장하려면?
    * 두개로 쪼개서 각 노드에 저장하고
    * 클라이언트가 요청할 때 다시 각각 가져다 쓰면 된다

* chunk server
    * 파일이 연속적 chunk로 나뉨
    * block size가 보통 64KB니까 chunk size도 16~64MB로 나눔
    * 각 chunk는 2~3개 정도로 duplicate 만들고 되도록 다른 rack들에 넣음
        * 왜 굳이 분배?
        * 하나의 rack은 power source 한개 ㅎㅎ
    * chunck server가 computation server이기도 함
* master node
    * 메타데이터 저장

### programming model

#### Word Count Problem

파일에 등장하는 단어들의 등장 횟수들을 분석하기

* 아마존 잘 팔리는 분석
* 유명한 url 찾기

solution

1. 모든 <word, count> pair가 mem에 들어가는 경우
    * ex. file 1TB
    * memory 4GB
    * 단어 100만개
    * 파일을 sequential하게 읽으면서 배열 업뎃 해주면 됨
2. <word, count> pair도 mem에 다 안들어갈 때
    * 단어가 10억 개 -> 16GB (왜지)
    * 아래의 관점에서 접근
    * `words doc.txt | sort | uniq -c`

### map reduce

* map(k, v) -> <k., v.>*
* reduce(k., <v.>*) -> <k., v..>*

1. input kv pairs 
    * usually key is ignored (like, file names)
2. map : intermediate kv pairs
3. group by keys : kv group
4. reduce : output kv pairs


